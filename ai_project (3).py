# -*- coding: utf-8 -*-
"""AI PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YgRUOIcDUIRRFvLeACrTxsNZpkj7P-MQ
"""

import random
from IPython.display import clear_output
import matplotlib.pyplot as plt
import numpy as np
import math

"""**Player Class:**

This class represents a generic player in the Tic-Tac-Toe game.
It has methods to initialize the player with a marker (X or O) and to display the current state of the board.
"""

class Player:
    # Initialize player with marker (X or O)
    def __init__(self, marker):
        self.marker = marker  # Set player's marker

    # Method to display the board
    def show_board(self, board):
        """
        Display the Tic-Tac-Toe board.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board
        """
        # Show the Tic-Tac-Toe board
        print('|'.join(board[0:3]))  # Display the first row of the board
        print('|'.join(board[3:6]))  # Display the second row of the board
        print('|'.join(board[6:9]))  # Display the third row of the board

"""**QLearningPlayer Class:**

This class represents a player that learns using the Q-learning algorithm.
It inherits from the Player class and implements methods to choose moves based on Q-values and update Q-values based on rewards.
"""

class QLearningPlayer(Player):
    # Initialize Q-learning player with marker, exploration rate (epsilon), learning rate (alpha), and discount factor (gamma)
    def __init__(self, marker, epsilon=0.4, alpha=0.3, gamma=0.9):
        """
        Initialize Q-learning player.

        Parameters:
        - marker: Marker (X or O) of the player
        - epsilon: Exploration rate (default: 0.4)
        - alpha: Learning rate (default: 0.3)
        - gamma: Discount factor (default: 0.9)
        """
        super().__init__(marker)  # Call the superclass constructor
        self.epsilon = epsilon    # Exploration rate: Probability of exploration (choosing a random action) instead of exploitation (choosing the best action)
        self.alpha = alpha        # Learning rate: Determines how much the Q-values are updated after each iteration
        self.gamma = gamma        # Discount factor: Determines the importance of future rewards in the Q-value update equation
        self.q_values = {}        # Dictionary to store Q-values: Maps (state, action) pairs to their corresponding Q-values

    # Get indices of available moves (empty cells) on the board
    def available_moves(self, board):
        """
        Get indices of available moves (empty cells) on the board.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board

        Returns:
        - List of indices of available (empty) cells on the board
        """
        return [i for i in range(9) if board[i] == ' ']  # Return indices of available (empty) cells on the board

    # Get Q-value for a given state-action pair
    def get_q_value(self, state, action):
        """
        Get Q-value for a given state-action pair.

        Parameters:
        - state: Tuple representing the current state of the Tic-Tac-Toe board
        - action: Index of the action (move) to be evaluated

        Returns:
        - Q-value for the given state-action pair, or 0 if not present
        """
        return self.q_values.get((state, action), 0)  # Return Q-value if exists, otherwise return 0

    # Choose an action based on epsilon-greedy policy
    def make_move(self, board):
        """
        Choose an action based on epsilon-greedy policy.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board

        Returns:
        - Index of the chosen action (move)
        """
        if random.random() < self.epsilon:
            return random.choice(self.available_moves(board))  # Randomly choose an action with probability epsilon
        else:
            best_move = None  # Initialize best move
            best_q_value = float('-inf')  # Initialize best Q-value with negative infinity
            # Iterate over available moves and choose the one with highest Q-value
            for action in self.available_moves(board):
                q_value = self.get_q_value(tuple(board), action)  # Get Q-value for current action
                if q_value > best_q_value:
                    best_move = action  # Update best move
                    best_q_value = q_value  # Update best Q-value
            return best_move  # Return the chosen action
    def make_move(board, row, col, player):
    if is_cell_empty(board, row, col):
        board[row][col] = player
        return True
    else:
        print("Invalid move! Try again.")
        return False

    # Update Q-value based on reward and next state using the Q-learning update rule
    def update_q_value(self, state, action, reward, next_state):
        """
        Update Q-value based on reward and next state using the Q-learning update rule.

        Parameters:
        - state: Tuple representing the current state of the Tic-Tac-Toe board
        - action: Index of the action (move) taken
        - reward: Reward received for taking the action
        - next_state: Tuple representing the next state of the Tic-Tac-Toe board
        """
        max_q_next_state = max([self.get_q_value(next_state, a) for a in self.available_moves(next_state)])  # Get maximum Q-value for next state
        old_q_value = self.get_q_value(state, action)  # Get current Q-value
        # Q-learning update rule
        self.q_values[(state, action)] = old_q_value + self.alpha * (reward + self.gamma * max_q_next_state - old_q_value)


    # Receive reward and update Q-value accordingly
    def reward(self, reward, state, action, next_state):
        """
        Receive reward and update Q-value accordingly.

        Parameters:
        - reward: Reward received for taking the action
        - state: Tuple representing the current state of the Tic-Tac-Toe board
        - action: Index of the action (move) taken
        - next_state: Tuple representing the next state of the Tic-Tac-Toe board
        """
        self.update_q_value(state, action, reward, next_state)  # Update Q-value based on received reward and next state

"""**MCMCPlayer Class:**

This class represents a player that uses Monte Carlo simulations to make decisions.
Similar to the QLearningPlayer class, it inherits from the Player class and implements a method to calculate win probabilities for each move.
"""

class MCMCPlayer(Player):
    AI_PLAYER_1 = 'X'  # Constant representing Player 1's marker
    AI_PLAYER_2 = 'O'  # Constant representing Player 2's marker

    def __init__(self, marker, num_simulations=1000):
        super().__init__(marker)  # Call the superclass constructor
        self.num_simulations = num_simulations  # Number of Monte Carlo simulations to run

    def available_moves(self, board):
        """
        Get indices of available moves (empty cells) on the board.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board

        Returns:
        - List of indices of available moves
        """
        return [i for i, cell in enumerate(board) if cell == ' ']  # Return indices of empty cells
        def is_board_full(board):
          for row in board:
            if ' ' in row:
              return False
              return True

    def calculate_win_probability(self, board, marker, move):
        """
        Calculate the win probability of making a move on the board.
        Uses Monte Carlo simulation to estimate the probability of winning after making the move.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board
        - marker: Marker (X or O) of the player making the move
        - move: Index of the move to be evaluated

        Returns:
        - Win probability of making the move
        """
        new_board = board[:]  # Copy the board
        new_board[move] = marker  # Make the move on the copied board

        wins = 0  # Initialize the number of wins to 0
        for _ in range(self.num_simulations):  # Perform Monte Carlo simulations
            sim_board = new_board[:]  # Copy the board for simulation
            random.shuffle(sim_board)  # Shuffle the board randomly
            if self.is_winner(sim_board, marker):  # Check if the marker wins in the simulation
                wins += 1  # Increment the number of wins
        return wins / self.num_simulations  # Return the win probability

    def simulate_opponent(board, player):
      empty_cells = [(i, j) for i in range(3) for j in range(3) if board[i][j] == ' ']
      return random.choice(empty_cells)

    def is_winner(self, board, marker):
        """
        Check if a player wins on the board.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board
        - marker: Marker (X or O) of the player

        Returns:
        - True if the player wins, False otherwise
        """
        return ((board[0] == board[1] == board[2] == marker) or
                (board[3] == board[4] == board[5] == marker) or
                (board[6] == board[7] == board[8] == marker) or
                (board[0] == board[3] == board[6] == marker) or
                (board[1] == board[4] == board[7] == marker) or
                (board[2] == board[5] == board[8] == marker) or
                (board[0] == board[4] == board[8] == marker) or
                (board[2] == board[4] == board[6] == marker))

    def make_move(self, board):
        """
        Determine the best move to make based on win probabilities.

        Parameters:
        - board: List representing the current state of the Tic-Tac-Toe board

        Returns:
        - Index of the best move to make
        """
        available_moves = self.available_moves(board)  # Get available moves on the board
        # Calculate win probabilities for each available move
        move_probabilities = [self.calculate_win_probability(board, self.marker, move) for move in available_moves]
        # Choose the move with the highest win probability
        best_move = available_moves[np.argmax(move_probabilities)]
        return best_move  # Return the index of the best move
        def update_model(board, player):
           pass

"""**TicTacToe Class:**

This class represents the Tic-Tac-Toe game environment.
It has methods to initialize the game, play multiple games, and check if the game is over and determine the winner.
"""

class TicTacToe:
    BLANK = ' '  # Constant representing a blank cell on the board
    AI_PLAYER_1 = 'X'  # Constant representing Player 1's marker
    AI_PLAYER_2 = 'O'  # Constant representing Player 2's marker
    REWARD_WIN = 10  # Reward for winning the game
    REWARD_LOSE = -10  # Reward for losing the game
    REWARD_TIE = 0  # Reward for a tie game

    def __init__(self, player1, player2):
        """
        Initialize the Tic-Tac-Toe game.

        Parameters:
        - player1: Instance of Player class for the first player
        - player2: Instance of Player class for the second player
        """
        self.player1 = player1  # Initialize player 1
        self.player2 = player2  # Initialize player 2
        self.first_player_turn = True  # Start with Player 1 as the first player
        self.board = [self.BLANK] * 9  # Initialize the game board
        self.wins_ai_player_1 = 0  # Initialize the number of wins for AI Player 1
        self.wins_ai_player_2 = 0  # Initialize the number of wins for AI Player 2

    def make_move(self, board):
        # Implement your AI logic here to determine the move
        # For demonstration, let's choose a random empty cell
        empty_cells = [i for i, cell in enumerate(board) if cell == TicTacToe.BLANK]
        return random.choice(empty_cells)

    def play(self, num_games):
        """
        Play multiple games of Tic-Tac-Toe.

        Parameters:
        - num_games: Number of games to play
        """
        for _ in range(num_games):
            while True:
                player = self.player1 if self.first_player_turn else self.player2  # Determine current player
                player_marker = self.AI_PLAYER_1 if self.first_player_turn else self.AI_PLAYER_2  # Determine current player's marker

                move = player.make_move(self.board)  # Get the move from the current player
                self.board[move] = player_marker  # Apply the move to the board

                game_over, winner = self.is_game_over()  # Check if the game is over and determine the winner

                if game_over:
                    if winner == self.AI_PLAYER_1:
                        self.wins_ai_player_1 += 1  # Increment win count for AI Player 1
                    elif winner == self.AI_PLAYER_2:
                        self.wins_ai_player_2 += 1  # Increment win count for AI Player 2
                    break

                self.first_player_turn = not self.first_player_turn  # Switch turns between players

        print("Wins for AI Player 1:", self.wins_ai_player_1)  # Print the total wins for AI Player 1
        print("Wins for AI Player 2:", self.wins_ai_player_2)  # Print the total wins for AI Player 2

    def is_game_over(self):
        """
        Check if the game is over and determine the winner.

        Returns:
        - Tuple indicating whether the game is over and the winner (if any)
        """
        for i in range(3):
            # Check rows and columns for a winning sequence
            if self.board[3 * i] == self.board[3 * i + 1] == self.board[3 * i + 2] != self.BLANK:
                return True, self.board[3 * i]  # Game over with a winner
            if self.board[i] == self.board[i + 3] == self.board[i + 6] != self.BLANK:
                return True, self.board[i]  # Game over with a winner

     def is_board_full(board):
       for row in board:
           if ' ' in row:
             return False
             return True

        # Check diagonals for a winning sequence
        if self.board[0] == self.board[4] == self.board[8] != self.BLANK:
            return True, self.board[0]  # Game over with a winner
        if self.board[2] == self.board[4] == self.board[6] != self.BLANK:
            return True, self.board[2]  # Game over with a winner

        if self.BLANK not in self.board:
            return True, None  # Game over with a tie

        return False, None  # Game is not over yet

"""Simulation and Evaluation:

The code runs simulations of the game between a Q-learning player and an MCMC player.
After each simulation, it counts the number of wins for each player and the number of tie games.
"""

num_simulations = 1000  # Number of simulations to run
q_learning_wins = 0  # Counter for wins by Q-learning player
mcmc_wins = 0  # Counter for wins by MCMC player
ties = 0  # Counter for tie games

# Run simulations
for _ in range(num_simulations):
    # Create instances of QLearningPlayer and MCMCPlayer for each simulation
    q_learning_player = QLearningPlayer(QLearningPlayer.AI_PLAYER_1)  # Q-learning player is Player 1
    mcmc_player = MCMCPlayer(MCMCPlayer.AI_PLAYER_2)  # MCMC player is Player 2
    # Initialize TicTacToe game with Q-learning player and MCMC player
    game = TicTacToe(q_learning_player, mcmc_player)
    # Play a single game in each simulation
    game.play(1)

    # Update win counters based on the outcome of the game
    if game.wins_ai_player_1 > game.wins_ai_player_2:
        q_learning_wins += 1  # Q-learning player wins
    elif game.wins_ai_player_2 > game.wins_ai_player_1:
        mcmc_wins += 1  # MCMC player wins
    else:
        ties += 1  # Tie game

# Print results
print("Wins for Q-learning Player:", q_learning_wins)  # Print number of wins by Q-learning player
print("Wins for MCMC Player:", mcmc_wins)  # Print number of wins by MCMC player
print("Number of Ties:", ties)  # Print number of tie games

"""Visualization:

The code calculates the average win rates for both players and plots a bar chart to visualize the results.
"""

# Define a function to run multiple simulations
def run_simulations(num_runs, num_simulations):
    """
    Run multiple simulations of Tic-Tac-Toe games between Q-learning and MCMC players.

    Parameters:
    - num_runs: Number of times to repeat the simulations
    - num_simulations: Number of simulations to run in each repetition

    Returns:
    - Lists of counts for wins by Q-learning player, wins by MCMC player, and tie games for each repetition
    """
    q_learning_wins_list = []  # List to store counts of wins by Q-learning player for each run
    mcmc_wins_list = []  # List to store counts of wins by MCMC player for each run
    ties_list = []  # List to store counts of tie games for each run

    for _ in range(num_runs):
        q_learning_wins = 0
        mcmc_wins = 0
        ties = 0

        for _ in range(num_simulations):
            q_learning_player = QLearningPlayer(QLearningPlayer.AI_PLAYER_1)  # Create Q-learning player instance
            mcmc_player = MCMCPlayer(MCMCPlayer.AI_PLAYER_2)  # Create MCMC player instance
            game = TicTacToe(q_learning_player, mcmc_player)  # Initialize Tic-Tac-Toe game
            game.play(1)  # Play a single game in each simulation

            # Update win counters based on the outcome of the game
            if game.wins_ai_player_1 > game.wins_ai_player_2:
                q_learning_wins += 1  # Increment win count for Q-learning player
            elif game.wins_ai_player_2 > game.wins_ai_player_1:
                mcmc_wins += 1  # Increment win count for MCMC player
            else:
                ties += 1  # Increment tie count

        q_learning_wins_list.append(q_learning_wins)  # Append win count for Q-learning player to list
        mcmc_wins_list.append(mcmc_wins)  # Append win count for MCMC player to list
        ties_list.append(ties)  # Append tie count to list

    return q_learning_wins_list, mcmc_wins_list, ties_list  # Return lists of win counts and tie counts

# Run simulations 10 times
num_runs = 5  # Number of times to repeat the simulations
num_simulations = 1000  # Number of simulations to run in each repetition
q_learning_wins_list, mcmc_wins_list, ties_list = run_simulations(num_runs, num_simulations)  # Call function to run simulations

# Calculate average win rates
avg_q_learning_wins = sum(q_learning_wins_list) / num_runs  # Calculate average wins for Q-learning player
avg_mcmc_wins = sum(mcmc_wins_list) / num_runs  # Calculate average wins for MCMC player
avg_ties = sum(ties_list) / num_runs  # Calculate average tie games

# Print average win rates
print("Average wins for Q-learning Player:", avg_q_learning_wins)
print("Average wins for MCMC Player:", avg_mcmc_wins)
print("Average ties:", avg_ties)

# Create a bar chart
players = ['Q-learning', 'MCMC', 'Ties']  # Labels for players
wins = [avg_q_learning_wins, avg_mcmc_wins, avg_ties]  # Average win counts for players

# Plot the bar chart
plt.bar(players, wins, color=['blue', 'green', 'orange'])  # Create the bar chart
plt.xlabel('Player')  # Set label for x-axis
plt.ylabel('Average Wins')  # Set label for y-axis
plt.title('Average Wins for Q-learning and MCMC Players')  # Set title for the plot
plt.show()  # Display the plot

